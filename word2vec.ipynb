{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da8fec61",
      "metadata": {},
      "source": [
        "# Word2Vec: Learning Word Embeddings\n",
        "\n",
        "Word2Vec is a neural network-based technique that learns dense vector representations (embeddings) of words from large text corpora. Unlike TF-IDF/BoW which treat words as independent symbols, Word2Vec captures **semantic relationships** — words with similar meanings have similar vectors.\n",
        "\n",
        "**Key concepts:**\n",
        "- **CBOW (Continuous Bag of Words)**: Predicts target word from surrounding context words\n",
        "- **Skip-gram**: Predicts context words from a target word\n",
        "- Words that appear in similar contexts get similar embeddings\n",
        "- Enables vector arithmetic: `king - man + woman ≈ queen`\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3deb06d",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade scipy numpy gensim -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1beb348",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec          # The Word2Vec model implementation\n",
        "import gensim.downloader as api              # Download pre-built corpora/models\n",
        "from gensim.utils import simple_preprocess   # Tokenization utility (lowercase, remove punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b386c7e",
      "metadata": {},
      "source": [
        "## 2. Load the 20 Newsgroups Dataset\n",
        "\n",
        "The **20 Newsgroups** corpus contains ~18,000 newsgroup posts across 20 different topics (politics, sports, religion, tech, etc.). It's a classic text classification benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0576c06",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the 20-newsgroups dataset via gensim's API\n",
        "# Returns an iterable of dicts with 'data' (text), 'topic', etc.\n",
        "corpus = api.load('20-newsgroups')\n",
        "\n",
        "# Convert to list so we can inspect and reuse it\n",
        "corpus_list = list(corpus)\n",
        "print(f\"Loaded {len(corpus_list)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebad8803",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect a sample document\n",
        "sample = corpus_list[0]\n",
        "print(f\"Keys: {sample.keys()}\")\n",
        "print(f\"Topic: {sample.get('topic', 'N/A')}\")\n",
        "print(f\"Text preview:\\n{sample['data'][:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "706aaf7f",
      "metadata": {},
      "source": [
        "## 3. Preprocess the Text\n",
        "\n",
        "`simple_preprocess()` does:\n",
        "- Lowercase everything\n",
        "- Remove punctuation and numbers\n",
        "- Tokenize into words\n",
        "- Filter tokens by length (default: 2-15 chars)\n",
        "\n",
        "Word2Vec expects input as a **list of tokenized sentences/documents** (list of lists of strings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2056b0f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize each document\n",
        "processed_docs = [simple_preprocess(doc['data']) for doc in corpus_list]\n",
        "\n",
        "print(f\"Processed {len(processed_docs)} documents\")\n",
        "print(f\"Sample tokenized doc: {processed_docs[0][:20]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c30b0573",
      "metadata": {},
      "source": [
        "## 4. Train the Word2Vec Model\n",
        "\n",
        "Key parameters:\n",
        "- **`min_count`**: Ignore words appearing fewer than N times (removes rare/noisy words)\n",
        "- **`vector_size`**: Dimensionality of word vectors (default 100)\n",
        "- **`window`**: Context window size (how many words around target to consider)\n",
        "- **`sg`**: 0=CBOW, 1=Skip-gram\n",
        "- **`workers`**: Parallel training threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e520d18e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Word2Vec model\n",
        "# min_count=3 filters out words appearing < 3 times (noise reduction)\n",
        "model = Word2Vec(\n",
        "    sentences=processed_docs,\n",
        "    vector_size=100,    # embedding dimension\n",
        "    window=5,           # context window\n",
        "    min_count=3,        # minimum word frequency\n",
        "    workers=4,          # parallel threads\n",
        "    sg=0                # 0=CBOW, 1=Skip-gram\n",
        ")\n",
        "\n",
        "print(f\"Vocabulary size: {len(model.wv)}\")\n",
        "print(f\"Vector dimensionality: {model.wv.vector_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa910940",
      "metadata": {},
      "source": [
        "## 5. Explore Word Embeddings\n",
        "\n",
        "Now the fun part — let's see what the model learned!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d9750d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access word vectors via model.wv (KeyedVectors object)\n",
        "wv = model.wv\n",
        "\n",
        "# Get the vector for a word\n",
        "print(\"Vector for 'computer':\")\n",
        "print(wv['computer'][:10], \"...\")  # First 10 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd19a9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find similar words (by cosine similarity)\n",
        "print(\"Words most similar to 'computer':\")\n",
        "wv.most_similar('computer', topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d45f508",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different words based on the 20-newsgroups topics\n",
        "print(\"Similar to 'science':\")\n",
        "print(wv.most_similar('science', topn=5))\n",
        "\n",
        "print(\"\\nSimilar to 'god':\")\n",
        "print(wv.most_similar('god', topn=5))\n",
        "\n",
        "print(\"\\nSimilar to 'windows':\") \n",
        "print(wv.most_similar('windows', topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "635d4fe9",
      "metadata": {},
      "source": [
        "## 6. Word Analogies (Vector Arithmetic)\n",
        "\n",
        "The classic Word2Vec demo: `king - man + woman = queen`\n",
        "\n",
        "This works because the model learns that the relationship between king/queen is similar to man/woman."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad197f7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word analogies: positive - negative\n",
        "# \"What is to X as Y is to Z?\"\n",
        "# Note: Results depend heavily on corpus size/domain — 20-newsgroups is relatively small\n",
        "\n",
        "try:\n",
        "    # mac - apple + microsoft = ?\n",
        "    result = wv.most_similar(positive=['mac', 'microsoft'], negative=['apple'], topn=3)\n",
        "    print(\"mac - apple + microsoft =\", result)\n",
        "except KeyError as e:\n",
        "    print(f\"Word not in vocabulary: {e}\")\n",
        "\n",
        "try:\n",
        "    # university - student + professor = ?\n",
        "    result = wv.most_similar(positive=['university', 'teach'], negative=['student'], topn=3)\n",
        "    print(\"university - student + teach =\", result)\n",
        "except KeyError as e:\n",
        "    print(f\"Word not in vocabulary: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "576fe78c",
      "metadata": {},
      "source": [
        "## 7. Word Similarity Scores\n",
        "\n",
        "Directly compute cosine similarity between word pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71af85d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Similarity between word pairs (cosine similarity, range -1 to 1)\n",
        "pairs = [\n",
        "    ('computer', 'software'),\n",
        "    ('computer', 'religion'),\n",
        "    ('science', 'research'),\n",
        "    ('god', 'jesus'),\n",
        "]\n",
        "\n",
        "for w1, w2 in pairs:\n",
        "    try:\n",
        "        sim = wv.similarity(w1, w2)\n",
        "        print(f\"{w1} <-> {w2}: {sim:.4f}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Word not found: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435e4744",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the odd one out (word that doesn't fit)\n",
        "try:\n",
        "    odd = wv.doesnt_match(['computer', 'software', 'hardware', 'god'])\n",
        "    print(f\"Doesn't match: {odd}\")\n",
        "except KeyError as e:\n",
        "    print(f\"Word not found: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b75af854",
      "metadata": {},
      "source": [
        "## 8. Save & Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39586fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the full model (can continue training later)\n",
        "model.save(\"word2vec_20newsgroups.model\")\n",
        "\n",
        "# Save just the word vectors (smaller, read-only)\n",
        "model.wv.save(\"word2vec_20newsgroups.wordvectors\")\n",
        "\n",
        "# Load them back\n",
        "# loaded_model = Word2Vec.load(\"word2vec_20newsgroups.model\")\n",
        "# loaded_wv = KeyedVectors.load(\"word2vec_20newsgroups.wordvectors\")\n",
        "\n",
        "print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead32edf",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "| TF-IDF/BoW | Word2Vec |\n",
        "|------------|----------|\n",
        "| Sparse, high-dimensional | Dense, low-dimensional (100-300d) |\n",
        "| Words are independent symbols | Words with similar meanings → similar vectors |\n",
        "| Explicit term frequency | Learned from context (neural network) |\n",
        "| Good for exact keyword matching | Good for semantic similarity |\n",
        "| No training required | Requires training on large corpus |\n",
        "\n",
        "**Next steps:**\n",
        "- Try pre-trained embeddings (e.g., `word2vec-google-news-300`)\n",
        "- Experiment with Skip-gram vs CBOW (`sg=1` vs `sg=0`)\n",
        "- Use embeddings for downstream tasks (classification, clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b1f192",
      "metadata": {},
      "source": [
        "## Bonus: Using Pre-trained Word Vectors\n",
        "\n",
        "Training on 20-newsgroups gives decent results, but pre-trained embeddings (trained on billions of words) are much better for general use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e18c83bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to download pre-trained Google News vectors (~1.7GB)\n",
        "# This gives MUCH better results for analogies like king-man+woman=queen\n",
        "\n",
        "# pretrained = api.load('word2vec-google-news-300')\n",
        "# print(pretrained.most_similar(positive=['king', 'woman'], negative=['man'], topn=3))\n",
        "\n",
        "# List available pre-trained models:\n",
        "print(\"Available pre-trained models:\")\n",
        "print([m for m in api.info()['models'].keys() if 'word2vec' in m.lower() or 'glove' in m.lower()])"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
