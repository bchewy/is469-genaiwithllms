{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2005586",
      "metadata": {},
      "source": [
        "# BERT: Contextual Word Embeddings\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP by introducing **contextual embeddings** — the same word gets different vector representations depending on its surrounding context.\n",
        "\n",
        "**Key differences from Word2Vec:**\n",
        "- **Word2Vec**: Static embeddings — \"bank\" always has the same vector\n",
        "- **BERT**: Contextual embeddings — \"river bank\" vs \"bank account\" produce different vectors for \"bank\"\n",
        "\n",
        "**Key concepts:**\n",
        "- **Transformer architecture**: Self-attention mechanism that considers all words simultaneously\n",
        "- **Bidirectional**: Considers both left and right context (unlike GPT which is left-to-right only)\n",
        "- **WordPiece tokenization**: Subword units handle unknown words gracefully\n",
        "- **[CLS] token**: Special token whose embedding represents the entire sequence\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7c7bd115",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers torch sentence-transformers scipy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eb826afa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/brianchew/dev/is469-genaiwithllms/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f4d4d9c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1016.86it/s, Materializing param=pooler.dense.weight]                              \n",
            "BertModel LOAD REPORT from: bert-base-uncased\n",
            "Key                                        | Status     |  | \n",
            "-------------------------------------------+------------+--+-\n",
            "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
            "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
            "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
            "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
            "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
            "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
            "cls.predictions.bias                       | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model config: 12 layers, 768 hidden size\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "# bert-base-uncased: 12 layers, 768 hidden size, 110M parameters\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.eval()  # Set to evaluation mode (disables dropout)\n",
        "\n",
        "print(f\"Model config: {model.config.num_hidden_layers} layers, {model.config.hidden_size} hidden size\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa311ec",
      "metadata": {},
      "source": [
        "### Alternative: Using AutoTokenizer (Class Style)\n",
        "\n",
        "Your class uses `AutoTokenizer` and `AutoModel` which auto-detect the right class. Also uses `bert-base-cased` (preserves case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65b85fe3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Class uses bert-base-cased (case-sensitive: \"Hello\" ≠ \"hello\")\n",
        "model_name = \"bert-base-cased\"\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "auto_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Loaded: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71654783",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize text (class style)\n",
        "text = \"Hello\"\n",
        "tokens = auto_tokenizer(text)\n",
        "\n",
        "print(\"Token dict:\", tokens)\n",
        "print(\"Input IDs:\", tokens['input_ids'])  # [101, 8667, 102] = [CLS], Hello, [SEP]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3047a041",
      "metadata": {},
      "source": [
        "### Accessing the Embedding Layer Directly\n",
        "\n",
        "BERT's embedding layer is a lookup table mapping token IDs → 768-dim vectors. This is the **static** part before contextualization happens in the transformer layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201a5d4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access the embedding layer (like in class)\n",
        "embedding_layer = auto_model.embeddings\n",
        "word_embeddings = embedding_layer.word_embeddings\n",
        "\n",
        "# The weight matrix: [vocab_size, hidden_size]\n",
        "print(\"Embedding weight shape:\", word_embeddings.weight.shape)\n",
        "# bert-base-cased has 28,996 tokens, each with 768 dimensions\n",
        "\n",
        "print(\"\\nFirst 5 rows of embedding matrix:\")\n",
        "print(word_embeddings.weight[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b839a26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get embedding for a specific token by ID\n",
        "# \"Hello\" has token_id 8667 in bert-base-cased\n",
        "token_id = auto_tokenizer.encode(\"Hello\", add_special_tokens=False)[0]\n",
        "print(f\"Token ID for 'Hello': {token_id}\")\n",
        "\n",
        "# Look up its embedding directly from the weight matrix\n",
        "hello_embedding = word_embeddings.weight[token_id]\n",
        "print(f\"Embedding shape: {hello_embedding.shape}\")\n",
        "print(f\"First 10 dims: {hello_embedding[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47315c45",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compare embeddings from the embedding layer (BEFORE contextualization)\n",
        "def get_token_embedding(word):\n",
        "    \"\"\"Get the raw embedding (non-contextual) for a word.\"\"\"\n",
        "    token_ids = auto_tokenizer.encode(word, add_special_tokens=False)\n",
        "    if len(token_ids) > 1:\n",
        "        print(f\"Warning: '{word}' splits into {len(token_ids)} tokens\")\n",
        "    emb = word_embeddings.weight[token_ids[0]].detach().numpy()\n",
        "    return emb.reshape(1, -1)\n",
        "\n",
        "# Compare some word pairs\n",
        "word_pairs = [\n",
        "    (\"Hello\", \"Hi\"),\n",
        "    (\"king\", \"queen\"),\n",
        "    (\"cat\", \"dog\"),\n",
        "    (\"cat\", \"computer\"),\n",
        "]\n",
        "\n",
        "print(\"Cosine similarity (raw embeddings, non-contextual):\\n\")\n",
        "for w1, w2 in word_pairs:\n",
        "    emb1 = get_token_embedding(w1)\n",
        "    emb2 = get_token_embedding(w2)\n",
        "    sim = cosine_similarity(emb1, emb2)[0][0]\n",
        "    print(f\"  {w1} <-> {w2}: {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52cc382d",
      "metadata": {},
      "source": [
        "**Note:** The embeddings above are from the **embedding layer** (static, like Word2Vec). The real power of BERT comes from passing these through 12 transformer layers to get **contextual** embeddings (shown in section 3)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae381776",
      "metadata": {},
      "source": [
        "## 2. BERT Tokenization (WordPiece)\n",
        "\n",
        "BERT uses **WordPiece** tokenization:\n",
        "- Common words stay intact: \"the\", \"cat\", \"running\"\n",
        "- Rare/unknown words split into subwords: \"embeddings\" → \"em\", \"##bed\", \"##ding\", \"##s\"\n",
        "- `##` prefix indicates a continuation of the previous token\n",
        "\n",
        "Special tokens:\n",
        "- `[CLS]`: Added at the start, its embedding represents the whole sequence\n",
        "- `[SEP]`: Separates sentences (used in sentence-pair tasks)\n",
        "- `[PAD]`: Padding for batch processing\n",
        "- `[MASK]`: Used during pre-training (masked language modeling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "32531a6c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: The cat sat on the mat.\n",
            "Tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
            "Token IDs: [101, 1996, 4937, 2938, 2006, 1996, 13523, 1012, 102]\n",
            "Decoded: [CLS] the cat sat on the mat. [SEP]\n"
          ]
        }
      ],
      "source": [
        "# Basic tokenization example\n",
        "text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Tokenize into subwords\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(f\"Original: {text}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "\n",
        "# Convert to token IDs (what the model actually sees)\n",
        "token_ids = tokenizer.encode(text)\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "\n",
        "# Decode back to text\n",
        "decoded = tokenizer.decode(token_ids)\n",
        "print(f\"Decoded: {decoded}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f8fa35a8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings                → ['em', '##bed', '##ding', '##s']\n",
            "unbelievable              → ['unbelievable']\n",
            "transformers              → ['transformers']\n",
            "antidisestablishment      → ['anti', '##dis', '##est', '##ab', '##lish', '##ment']\n",
            "ChatGPT                   → ['chat', '##gp', '##t']\n"
          ]
        }
      ],
      "source": [
        "# Subword splitting for complex/rare words\n",
        "examples = [\n",
        "    \"embeddings\",           # Technical term\n",
        "    \"unbelievable\",         # Long word with prefixes\n",
        "    \"transformers\",         # Domain-specific\n",
        "    \"antidisestablishment\", # Very long word\n",
        "    \"ChatGPT\",              # Modern term (not in BERT's vocab)\n",
        "]\n",
        "\n",
        "for word in examples:\n",
        "    tokens = tokenizer.tokenize(word)\n",
        "    print(f\"{word:25} → {tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0dd383f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102]])\n",
            "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Tokens: ['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "# Full tokenization with special tokens\n",
        "text = \"Hello, how are you?\"\n",
        "\n",
        "# Using the tokenizer to get model inputs\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "print(\"Input IDs:\", inputs['input_ids'])\n",
        "print(\"Attention mask:\", inputs['attention_mask'])\n",
        "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
        "\n",
        "# Note: [CLS] at start (101), [SEP] at end (102)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b76e01de",
      "metadata": {},
      "source": [
        "## 3. Extracting Word Embeddings\n",
        "\n",
        "BERT outputs a 768-dimensional vector for each token. The key insight: **the same word gets different embeddings based on context**.\n",
        "\n",
        "The output shape is `[batch_size, sequence_length, hidden_size]` where:\n",
        "- `batch_size`: Number of sentences (1 for single sentence)\n",
        "- `sequence_length`: Number of tokens including [CLS] and [SEP]\n",
        "- `hidden_size`: 768 for bert-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b5bf53e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embeddings(text):\n",
        "    \"\"\"Get BERT embeddings for all tokens in text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    \n",
        "    with torch.no_grad():  # No gradient computation needed for inference\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # last_hidden_state: [batch, seq_len, 768]\n",
        "    embeddings = outputs.last_hidden_state\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    \n",
        "    return tokens, embeddings[0]  # Return tokens and embeddings for first (only) batch item\n",
        "\n",
        "\n",
        "def get_word_embedding(text, target_word):\n",
        "    \"\"\"Get embedding for a specific word in the text.\"\"\"\n",
        "    tokens, embeddings = get_embeddings(text)\n",
        "    \n",
        "    # Find the target word in tokens (handle subwords by looking for exact match)\n",
        "    target_lower = target_word.lower()\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token == target_lower:\n",
        "            return embeddings[i]\n",
        "    \n",
        "    # If not found as exact token, might be a subword - find starting token\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token.startswith(target_lower[:3]):  # Partial match for subwords\n",
        "            return embeddings[i]\n",
        "    \n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3e0ca2c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['[CLS]', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', '[SEP]']\n",
            "Embedding shape: torch.Size([12, 768])\n",
            "\n",
            "First token '[CLS]' embedding (first 10 dims):\n",
            "tensor([-0.3608,  0.2271, -0.3030, -0.1880,  0.0475, -0.3690,  0.1099,  0.3495,\n",
            "        -0.1674, -0.2070])\n"
          ]
        }
      ],
      "source": [
        "# Get embeddings for a sentence\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens, embeddings = get_embeddings(text)\n",
        "\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Embedding shape: {embeddings.shape}\")  # [seq_len, 768]\n",
        "print(f\"\\nFirst token '[CLS]' embedding (first 10 dims):\")\n",
        "print(embeddings[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50048fd6",
      "metadata": {},
      "source": [
        "### Contextual Embeddings Demo\n",
        "\n",
        "The magic of BERT: **same word, different contexts → different embeddings**\n",
        "\n",
        "Let's compare \"bank\" in:\n",
        "1. \"I deposited money at the bank\" (financial institution)\n",
        "2. \"I sat by the river bank\" (edge of water)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9022d035",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: 'I deposited money at the bank'\n",
            "Sentence 2: 'I sat by the river bank'\n",
            "\n",
            "Cosine similarity between 'bank' embeddings: 0.7393\n",
            "\n",
            "→ Different contexts produce different embeddings for the same word!\n"
          ]
        }
      ],
      "source": [
        "# Two sentences with \"bank\" in different contexts\n",
        "sent1 = \"I deposited money at the bank\"\n",
        "sent2 = \"I sat by the river bank\"\n",
        "\n",
        "# Get embeddings for \"bank\" in each context\n",
        "bank_financial = get_word_embedding(sent1, \"bank\")\n",
        "bank_river = get_word_embedding(sent2, \"bank\")\n",
        "\n",
        "# Compute cosine similarity between the two \"bank\" embeddings\n",
        "# cosine() returns distance (0 = identical), so similarity = 1 - distance\n",
        "similarity = 1 - cosine(bank_financial.numpy(), bank_river.numpy())\n",
        "\n",
        "print(f\"Sentence 1: '{sent1}'\")\n",
        "print(f\"Sentence 2: '{sent2}'\")\n",
        "print(f\"\\nCosine similarity between 'bank' embeddings: {similarity:.4f}\")\n",
        "print(\"\\n→ Different contexts produce different embeddings for the same word!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "612dedf1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity for same word in different contexts:\n",
            "\n",
            "'apple': 0.1962\n",
            "  • I ate a delicious apple\n",
            "  • Apple released a new iPhone\n",
            "\n",
            "'bat': 0.4594\n",
            "  • The bat flew out of the cave\n",
            "  • He swung the bat and hit a home run\n",
            "\n",
            "'cell': 0.3688\n",
            "  • The prisoner was locked in a cell\n",
            "  • The cell divides during mitosis\n",
            "\n",
            "'python': 0.4948\n",
            "  • A python is a large snake\n",
            "  • I wrote the code in Python\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# More examples of contextual embeddings\n",
        "word_contexts = [\n",
        "    (\"apple\", \"I ate a delicious apple\", \"Apple released a new iPhone\"),\n",
        "    (\"bat\", \"The bat flew out of the cave\", \"He swung the bat and hit a home run\"),\n",
        "    (\"cell\", \"The prisoner was locked in a cell\", \"The cell divides during mitosis\"),\n",
        "    (\"python\", \"A python is a large snake\", \"I wrote the code in Python\"),\n",
        "]\n",
        "\n",
        "print(\"Cosine similarity for same word in different contexts:\\n\")\n",
        "for word, sent1, sent2 in word_contexts:\n",
        "    emb1 = get_word_embedding(sent1, word)\n",
        "    emb2 = get_word_embedding(sent2, word)\n",
        "    \n",
        "    if emb1 is not None and emb2 is not None:\n",
        "        sim = 1 - cosine(emb1.numpy(), emb2.numpy())\n",
        "        print(f\"'{word}': {sim:.4f}\")\n",
        "        print(f\"  • {sent1}\")\n",
        "        print(f\"  • {sent2}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d48cd8",
      "metadata": {},
      "source": [
        "## 4. Sentence Embeddings\n",
        "\n",
        "For many tasks, we need a single vector representing the entire sentence. Two common approaches:\n",
        "\n",
        "1. **[CLS] token embedding**: The first token is designed to aggregate sequence information\n",
        "2. **Mean pooling**: Average all token embeddings (often works better)\n",
        "\n",
        "For production use, **Sentence-Transformers** provides models fine-tuned specifically for sentence similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "31e63698",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sentence_embedding_cls(text):\n",
        "    \"\"\"Get sentence embedding using [CLS] token.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # [CLS] is the first token (index 0)\n",
        "    return outputs.last_hidden_state[0, 0, :]\n",
        "\n",
        "\n",
        "def get_sentence_embedding_mean(text):\n",
        "    \"\"\"Get sentence embedding using mean pooling.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Average all token embeddings (excluding [CLS] and [SEP] for cleaner results)\n",
        "    embeddings = outputs.last_hidden_state[0, 1:-1, :]  # Skip first and last tokens\n",
        "    return embeddings.mean(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "28157c3a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence similarity using [CLS] vs Mean pooling:\n",
            "\n",
            "[CLS] token method:\n",
            "  'The weather is beautiful today...' <-> 'It's a lovely sunny day....': 0.9552\n",
            "  'The weather is beautiful today...' <-> 'I need to buy groceries....': 0.8398\n",
            "  'It's a lovely sunny day....' <-> 'I need to buy groceries....': 0.8608\n",
            "\n",
            "Mean pooling method:\n",
            "  'The weather is beautiful today...' <-> 'It's a lovely sunny day....': 0.8432\n",
            "  'The weather is beautiful today...' <-> 'I need to buy groceries....': 0.5501\n",
            "  'It's a lovely sunny day....' <-> 'I need to buy groceries....': 0.5491\n"
          ]
        }
      ],
      "source": [
        "# Compare the two methods\n",
        "sentences = [\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"It's a lovely sunny day.\",\n",
        "    \"I need to buy groceries.\",\n",
        "]\n",
        "\n",
        "print(\"Sentence similarity using [CLS] vs Mean pooling:\\n\")\n",
        "\n",
        "# CLS method\n",
        "cls_embs = [get_sentence_embedding_cls(s) for s in sentences]\n",
        "print(\"[CLS] token method:\")\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(i + 1, len(sentences)):\n",
        "        sim = 1 - cosine(cls_embs[i].numpy(), cls_embs[j].numpy())\n",
        "        print(f\"  '{sentences[i][:30]}...' <-> '{sentences[j][:30]}...': {sim:.4f}\")\n",
        "\n",
        "# Mean pooling method\n",
        "print(\"\\nMean pooling method:\")\n",
        "mean_embs = [get_sentence_embedding_mean(s) for s in sentences]\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(i + 1, len(sentences)):\n",
        "        sim = 1 - cosine(mean_embs[i].numpy(), mean_embs[j].numpy())\n",
        "        print(f\"  '{sentences[i][:30]}...' <-> '{sentences[j][:30]}...': {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3ca15a",
      "metadata": {},
      "source": [
        "### Sentence-Transformers (Recommended)\n",
        "\n",
        "For sentence similarity tasks, use **Sentence-Transformers** — BERT models fine-tuned specifically for semantic similarity. Much better results than raw BERT!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3c60d4ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1281.62it/s, Materializing param=pooler.dense.weight]                             \n",
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape: (3, 384)\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a sentence-transformer model (all-MiniLM-L6-v2 is fast and good)\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode sentences - super simple API!\n",
        "sentences = [\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"It's a lovely sunny day.\",\n",
        "    \"I need to buy groceries.\",\n",
        "]\n",
        "\n",
        "embeddings = st_model.encode(sentences)\n",
        "print(f\"Embedding shape: {embeddings.shape}\")  # [num_sentences, 384]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "febf2bbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence similarity matrix (Sentence-Transformers):\n",
            "\n",
            "0: The weather is beautiful today.\n",
            "1: It's a lovely sunny day.\n",
            "2: I need to buy groceries.\n",
            "\n",
            "tensor([[1.0000, 0.7101, 0.0845],\n",
            "        [0.7101, 1.0000, 0.0708],\n",
            "        [0.0845, 0.0708, 1.0000]])\n"
          ]
        }
      ],
      "source": [
        "# Compute similarities using sentence-transformers\n",
        "from sentence_transformers import util\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cos_sim = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "print(\"Sentence similarity matrix (Sentence-Transformers):\\n\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"{i}: {sent[:40]}\")\n",
        "\n",
        "print(f\"\\n{cos_sim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08dc793",
      "metadata": {},
      "source": [
        "## 5. Semantic Search\n",
        "\n",
        "A practical application: find the most similar documents to a query. Unlike keyword search (TF-IDF), semantic search understands meaning — \"automobile\" matches \"car\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e7b6fbf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Document corpus\n",
        "documents = [\n",
        "    \"The cat sits on the windowsill watching birds.\",\n",
        "    \"Machine learning models require large datasets.\",\n",
        "    \"Python is a popular programming language.\",\n",
        "    \"The dog runs in the park chasing squirrels.\",\n",
        "    \"Deep learning uses neural networks with many layers.\",\n",
        "    \"JavaScript is used for web development.\",\n",
        "    \"The kitten plays with a ball of yarn.\",\n",
        "    \"Natural language processing analyzes text data.\",\n",
        "]\n",
        "\n",
        "# Encode all documents\n",
        "doc_embeddings = st_model.encode(documents)\n",
        "\n",
        "def semantic_search(query, top_k=3):\n",
        "    \"\"\"Find most similar documents to query.\"\"\"\n",
        "    query_embedding = st_model.encode([query])\n",
        "    similarities = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
        "    \n",
        "    # Get top-k indices\n",
        "    top_indices = similarities.argsort(descending=True)[:top_k]\n",
        "    \n",
        "    print(f\"Query: '{query}'\\n\")\n",
        "    print(\"Top matches:\")\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        print(f\"  {i+1}. [{similarities[idx]:.4f}] {documents[idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "566c39d2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 'feline animals'\n",
            "\n",
            "Top matches:\n",
            "  1. [0.3616] The kitten plays with a ball of yarn.\n",
            "  2. [0.3593] The cat sits on the windowsill watching birds.\n",
            "  3. [0.2148] The dog runs in the park chasing squirrels.\n"
          ]
        }
      ],
      "source": [
        "# Try different queries - notice semantic understanding!\n",
        "semantic_search(\"feline animals\")  # Should match cat/kitten docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4bcd3582",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 'artificial intelligence'\n",
            "\n",
            "Top matches:\n",
            "  1. [0.3811] Deep learning uses neural networks with many layers.\n",
            "  2. [0.3129] Machine learning models require large datasets.\n",
            "  3. [0.3117] Python is a popular programming language.\n"
          ]
        }
      ],
      "source": [
        "semantic_search(\"artificial intelligence\")  # Should match ML/DL docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ba21a57f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 'coding languages'\n",
            "\n",
            "Top matches:\n",
            "  1. [0.5095] Python is a popular programming language.\n",
            "  2. [0.3871] JavaScript is used for web development.\n",
            "  3. [0.2497] Deep learning uses neural networks with many layers.\n"
          ]
        }
      ],
      "source": [
        "semantic_search(\"coding languages\")  # Should match Python/JavaScript docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36f2ac0",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: Word2Vec vs BERT\n",
        "\n",
        "| Aspect | Word2Vec | BERT |\n",
        "|--------|----------|------|\n",
        "| **Embedding type** | Static (one vector per word) | Contextual (different vectors based on context) |\n",
        "| **Architecture** | Shallow neural network | Deep Transformer (12+ layers) |\n",
        "| **Training objective** | Predict context/target words | Masked language modeling + next sentence |\n",
        "| **Handles polysemy?** | No — \"bank\" is always same vector | Yes — \"river bank\" ≠ \"bank account\" |\n",
        "| **Unknown words** | Out-of-vocabulary (OOV) problem | WordPiece handles any word |\n",
        "| **Dimensions** | Typically 100-300 | 768 (base) or 1024 (large) |\n",
        "| **Speed** | Very fast | Slower (more computation) |\n",
        "| **Pre-training data** | Often domain-specific | Massive web corpus (Wikipedia, books) |\n",
        "| **Best for** | Simple similarity, limited compute | Complex NLU, semantic understanding |\n",
        "\n",
        "**The embedding evolution:**\n",
        "1. **TF-IDF/BoW**: Sparse, word-independent\n",
        "2. **Word2Vec**: Dense, but static\n",
        "3. **BERT**: Dense and contextual — the current standard\n",
        "\n",
        "**Next steps:**\n",
        "- Try different BERT variants (RoBERTa, DistilBERT, ALBERT)\n",
        "- Fine-tune BERT for specific tasks (classification, NER, QA)\n",
        "- Explore newer models (GPT, T5, LLaMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d1d4d7",
      "metadata": {},
      "source": [
        "## Bonus: Visualizing Embeddings\n",
        "\n",
        "Reduce 768 dimensions to 2D to see how sentences cluster by meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "449126b3",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Sentences grouped by topic\u001b[39;00m\n\u001b[32m      5\u001b[39m sentences = [\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Animals\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe cat sleeps on the couch.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSnow covers the ground.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m ]\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sentences grouped by topic\n",
        "sentences = [\n",
        "    # Animals\n",
        "    \"The cat sleeps on the couch.\",\n",
        "    \"A dog plays in the yard.\",\n",
        "    \"The kitten chases a mouse.\",\n",
        "    # Programming\n",
        "    \"Python is great for data science.\",\n",
        "    \"JavaScript powers the web.\",\n",
        "    \"Rust is a systems language.\",\n",
        "    # Weather\n",
        "    \"It's raining outside today.\",\n",
        "    \"The sun is shining brightly.\",\n",
        "    \"Snow covers the ground.\",\n",
        "]\n",
        "\n",
        "labels = [\"animal\"] * 3 + [\"programming\"] * 3 + [\"weather\"] * 3\n",
        "colors = {\"animal\": \"red\", \"programming\": \"blue\", \"weather\": \"green\"}\n",
        "\n",
        "# Get embeddings\n",
        "embeddings = st_model.encode(sentences)\n",
        "\n",
        "# Reduce to 2D with PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(embeddings)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, (x, y) in enumerate(reduced):\n",
        "    plt.scatter(x, y, c=colors[labels[i]], s=100)\n",
        "    plt.annotate(sentences[i][:25] + \"...\", (x, y), fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.title(\"BERT Sentence Embeddings (PCA)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "\n",
        "# Legend\n",
        "for label, color in colors.items():\n",
        "    plt.scatter([], [], c=color, label=label)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
