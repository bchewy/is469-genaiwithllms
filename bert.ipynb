{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2005586",
      "metadata": {},
      "source": [
        "# BERT: Contextual Word Embeddings\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP by introducing **contextual embeddings** — the same word gets different vector representations depending on its surrounding context.\n",
        "\n",
        "**Key differences from Word2Vec:**\n",
        "- **Word2Vec**: Static embeddings — \"bank\" always has the same vector\n",
        "- **BERT**: Contextual embeddings — \"river bank\" vs \"bank account\" produce different vectors for \"bank\"\n",
        "\n",
        "**Key concepts:**\n",
        "- **Transformer architecture**: Self-attention mechanism that considers all words simultaneously\n",
        "- **Bidirectional**: Considers both left and right context (unlike GPT which is left-to-right only)\n",
        "- **WordPiece tokenization**: Subword units handle unknown words gracefully\n",
        "- **[CLS] token**: Special token whose embedding represents the entire sequence\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7bd115",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers torch sentence-transformers scipy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb826afa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d4d9c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "# bert-base-uncased: 12 layers, 768 hidden size, 110M parameters\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.eval()  # Set to evaluation mode (disables dropout)\n",
        "\n",
        "print(f\"Model config: {model.config.num_hidden_layers} layers, {model.config.hidden_size} hidden size\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae381776",
      "metadata": {},
      "source": [
        "## 2. BERT Tokenization (WordPiece)\n",
        "\n",
        "BERT uses **WordPiece** tokenization:\n",
        "- Common words stay intact: \"the\", \"cat\", \"running\"\n",
        "- Rare/unknown words split into subwords: \"embeddings\" → \"em\", \"##bed\", \"##ding\", \"##s\"\n",
        "- `##` prefix indicates a continuation of the previous token\n",
        "\n",
        "Special tokens:\n",
        "- `[CLS]`: Added at the start, its embedding represents the whole sequence\n",
        "- `[SEP]`: Separates sentences (used in sentence-pair tasks)\n",
        "- `[PAD]`: Padding for batch processing\n",
        "- `[MASK]`: Used during pre-training (masked language modeling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32531a6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic tokenization example\n",
        "text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Tokenize into subwords\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(f\"Original: {text}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "\n",
        "# Convert to token IDs (what the model actually sees)\n",
        "token_ids = tokenizer.encode(text)\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "\n",
        "# Decode back to text\n",
        "decoded = tokenizer.decode(token_ids)\n",
        "print(f\"Decoded: {decoded}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8fa35a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Subword splitting for complex/rare words\n",
        "examples = [\n",
        "    \"embeddings\",           # Technical term\n",
        "    \"unbelievable\",         # Long word with prefixes\n",
        "    \"transformers\",         # Domain-specific\n",
        "    \"antidisestablishment\", # Very long word\n",
        "    \"ChatGPT\",              # Modern term (not in BERT's vocab)\n",
        "]\n",
        "\n",
        "for word in examples:\n",
        "    tokens = tokenizer.tokenize(word)\n",
        "    print(f\"{word:25} → {tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd383f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full tokenization with special tokens\n",
        "text = \"Hello, how are you?\"\n",
        "\n",
        "# Using the tokenizer to get model inputs\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "print(\"Input IDs:\", inputs['input_ids'])\n",
        "print(\"Attention mask:\", inputs['attention_mask'])\n",
        "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
        "\n",
        "# Note: [CLS] at start (101), [SEP] at end (102)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b76e01de",
      "metadata": {},
      "source": [
        "## 3. Extracting Word Embeddings\n",
        "\n",
        "BERT outputs a 768-dimensional vector for each token. The key insight: **the same word gets different embeddings based on context**.\n",
        "\n",
        "The output shape is `[batch_size, sequence_length, hidden_size]` where:\n",
        "- `batch_size`: Number of sentences (1 for single sentence)\n",
        "- `sequence_length`: Number of tokens including [CLS] and [SEP]\n",
        "- `hidden_size`: 768 for bert-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5bf53e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embeddings(text):\n",
        "    \"\"\"Get BERT embeddings for all tokens in text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    \n",
        "    with torch.no_grad():  # No gradient computation needed for inference\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # last_hidden_state: [batch, seq_len, 768]\n",
        "    embeddings = outputs.last_hidden_state\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    \n",
        "    return tokens, embeddings[0]  # Return tokens and embeddings for first (only) batch item\n",
        "\n",
        "\n",
        "def get_word_embedding(text, target_word):\n",
        "    \"\"\"Get embedding for a specific word in the text.\"\"\"\n",
        "    tokens, embeddings = get_embeddings(text)\n",
        "    \n",
        "    # Find the target word in tokens (handle subwords by looking for exact match)\n",
        "    target_lower = target_word.lower()\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token == target_lower:\n",
        "            return embeddings[i]\n",
        "    \n",
        "    # If not found as exact token, might be a subword - find starting token\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token.startswith(target_lower[:3]):  # Partial match for subwords\n",
        "            return embeddings[i]\n",
        "    \n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0ca2c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get embeddings for a sentence\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens, embeddings = get_embeddings(text)\n",
        "\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Embedding shape: {embeddings.shape}\")  # [seq_len, 768]\n",
        "print(f\"\\nFirst token '[CLS]' embedding (first 10 dims):\")\n",
        "print(embeddings[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50048fd6",
      "metadata": {},
      "source": [
        "### Contextual Embeddings Demo\n",
        "\n",
        "The magic of BERT: **same word, different contexts → different embeddings**\n",
        "\n",
        "Let's compare \"bank\" in:\n",
        "1. \"I deposited money at the bank\" (financial institution)\n",
        "2. \"I sat by the river bank\" (edge of water)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9022d035",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Two sentences with \"bank\" in different contexts\n",
        "sent1 = \"I deposited money at the bank\"\n",
        "sent2 = \"I sat by the river bank\"\n",
        "\n",
        "# Get embeddings for \"bank\" in each context\n",
        "bank_financial = get_word_embedding(sent1, \"bank\")\n",
        "bank_river = get_word_embedding(sent2, \"bank\")\n",
        "\n",
        "# Compute cosine similarity between the two \"bank\" embeddings\n",
        "# cosine() returns distance (0 = identical), so similarity = 1 - distance\n",
        "similarity = 1 - cosine(bank_financial.numpy(), bank_river.numpy())\n",
        "\n",
        "print(f\"Sentence 1: '{sent1}'\")\n",
        "print(f\"Sentence 2: '{sent2}'\")\n",
        "print(f\"\\nCosine similarity between 'bank' embeddings: {similarity:.4f}\")\n",
        "print(\"\\n→ Different contexts produce different embeddings for the same word!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612dedf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# More examples of contextual embeddings\n",
        "word_contexts = [\n",
        "    (\"apple\", \"I ate a delicious apple\", \"Apple released a new iPhone\"),\n",
        "    (\"bat\", \"The bat flew out of the cave\", \"He swung the bat and hit a home run\"),\n",
        "    (\"cell\", \"The prisoner was locked in a cell\", \"The cell divides during mitosis\"),\n",
        "    (\"python\", \"A python is a large snake\", \"I wrote the code in Python\"),\n",
        "]\n",
        "\n",
        "print(\"Cosine similarity for same word in different contexts:\\n\")\n",
        "for word, sent1, sent2 in word_contexts:\n",
        "    emb1 = get_word_embedding(sent1, word)\n",
        "    emb2 = get_word_embedding(sent2, word)\n",
        "    \n",
        "    if emb1 is not None and emb2 is not None:\n",
        "        sim = 1 - cosine(emb1.numpy(), emb2.numpy())\n",
        "        print(f\"'{word}': {sim:.4f}\")\n",
        "        print(f\"  • {sent1}\")\n",
        "        print(f\"  • {sent2}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d48cd8",
      "metadata": {},
      "source": [
        "## 4. Sentence Embeddings\n",
        "\n",
        "For many tasks, we need a single vector representing the entire sentence. Two common approaches:\n",
        "\n",
        "1. **[CLS] token embedding**: The first token is designed to aggregate sequence information\n",
        "2. **Mean pooling**: Average all token embeddings (often works better)\n",
        "\n",
        "For production use, **Sentence-Transformers** provides models fine-tuned specifically for sentence similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e63698",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sentence_embedding_cls(text):\n",
        "    \"\"\"Get sentence embedding using [CLS] token.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # [CLS] is the first token (index 0)\n",
        "    return outputs.last_hidden_state[0, 0, :]\n",
        "\n",
        "\n",
        "def get_sentence_embedding_mean(text):\n",
        "    \"\"\"Get sentence embedding using mean pooling.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Average all token embeddings (excluding [CLS] and [SEP] for cleaner results)\n",
        "    embeddings = outputs.last_hidden_state[0, 1:-1, :]  # Skip first and last tokens\n",
        "    return embeddings.mean(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28157c3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare the two methods\n",
        "sentences = [\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"It's a lovely sunny day.\",\n",
        "    \"I need to buy groceries.\",\n",
        "]\n",
        "\n",
        "print(\"Sentence similarity using [CLS] vs Mean pooling:\\n\")\n",
        "\n",
        "# CLS method\n",
        "cls_embs = [get_sentence_embedding_cls(s) for s in sentences]\n",
        "print(\"[CLS] token method:\")\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(i + 1, len(sentences)):\n",
        "        sim = 1 - cosine(cls_embs[i].numpy(), cls_embs[j].numpy())\n",
        "        print(f\"  '{sentences[i][:30]}...' <-> '{sentences[j][:30]}...': {sim:.4f}\")\n",
        "\n",
        "# Mean pooling method\n",
        "print(\"\\nMean pooling method:\")\n",
        "mean_embs = [get_sentence_embedding_mean(s) for s in sentences]\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(i + 1, len(sentences)):\n",
        "        sim = 1 - cosine(mean_embs[i].numpy(), mean_embs[j].numpy())\n",
        "        print(f\"  '{sentences[i][:30]}...' <-> '{sentences[j][:30]}...': {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3ca15a",
      "metadata": {},
      "source": [
        "### Sentence-Transformers (Recommended)\n",
        "\n",
        "For sentence similarity tasks, use **Sentence-Transformers** — BERT models fine-tuned specifically for semantic similarity. Much better results than raw BERT!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c60d4ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a sentence-transformer model (all-MiniLM-L6-v2 is fast and good)\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode sentences - super simple API!\n",
        "sentences = [\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"It's a lovely sunny day.\",\n",
        "    \"I need to buy groceries.\",\n",
        "]\n",
        "\n",
        "embeddings = st_model.encode(sentences)\n",
        "print(f\"Embedding shape: {embeddings.shape}\")  # [num_sentences, 384]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "febf2bbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute similarities using sentence-transformers\n",
        "from sentence_transformers import util\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cos_sim = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "print(\"Sentence similarity matrix (Sentence-Transformers):\\n\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"{i}: {sent[:40]}\")\n",
        "\n",
        "print(f\"\\n{cos_sim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08dc793",
      "metadata": {},
      "source": [
        "## 5. Semantic Search\n",
        "\n",
        "A practical application: find the most similar documents to a query. Unlike keyword search (TF-IDF), semantic search understands meaning — \"automobile\" matches \"car\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b6fbf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Document corpus\n",
        "documents = [\n",
        "    \"The cat sits on the windowsill watching birds.\",\n",
        "    \"Machine learning models require large datasets.\",\n",
        "    \"Python is a popular programming language.\",\n",
        "    \"The dog runs in the park chasing squirrels.\",\n",
        "    \"Deep learning uses neural networks with many layers.\",\n",
        "    \"JavaScript is used for web development.\",\n",
        "    \"The kitten plays with a ball of yarn.\",\n",
        "    \"Natural language processing analyzes text data.\",\n",
        "]\n",
        "\n",
        "# Encode all documents\n",
        "doc_embeddings = st_model.encode(documents)\n",
        "\n",
        "def semantic_search(query, top_k=3):\n",
        "    \"\"\"Find most similar documents to query.\"\"\"\n",
        "    query_embedding = st_model.encode([query])\n",
        "    similarities = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
        "    \n",
        "    # Get top-k indices\n",
        "    top_indices = similarities.argsort(descending=True)[:top_k]\n",
        "    \n",
        "    print(f\"Query: '{query}'\\n\")\n",
        "    print(\"Top matches:\")\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        print(f\"  {i+1}. [{similarities[idx]:.4f}] {documents[idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "566c39d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different queries - notice semantic understanding!\n",
        "semantic_search(\"feline animals\")  # Should match cat/kitten docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bcd3582",
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_search(\"artificial intelligence\")  # Should match ML/DL docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba21a57f",
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_search(\"coding languages\")  # Should match Python/JavaScript docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36f2ac0",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: Word2Vec vs BERT\n",
        "\n",
        "| Aspect | Word2Vec | BERT |\n",
        "|--------|----------|------|\n",
        "| **Embedding type** | Static (one vector per word) | Contextual (different vectors based on context) |\n",
        "| **Architecture** | Shallow neural network | Deep Transformer (12+ layers) |\n",
        "| **Training objective** | Predict context/target words | Masked language modeling + next sentence |\n",
        "| **Handles polysemy?** | No — \"bank\" is always same vector | Yes — \"river bank\" ≠ \"bank account\" |\n",
        "| **Unknown words** | Out-of-vocabulary (OOV) problem | WordPiece handles any word |\n",
        "| **Dimensions** | Typically 100-300 | 768 (base) or 1024 (large) |\n",
        "| **Speed** | Very fast | Slower (more computation) |\n",
        "| **Pre-training data** | Often domain-specific | Massive web corpus (Wikipedia, books) |\n",
        "| **Best for** | Simple similarity, limited compute | Complex NLU, semantic understanding |\n",
        "\n",
        "**The embedding evolution:**\n",
        "1. **TF-IDF/BoW**: Sparse, word-independent\n",
        "2. **Word2Vec**: Dense, but static\n",
        "3. **BERT**: Dense and contextual — the current standard\n",
        "\n",
        "**Next steps:**\n",
        "- Try different BERT variants (RoBERTa, DistilBERT, ALBERT)\n",
        "- Fine-tune BERT for specific tasks (classification, NER, QA)\n",
        "- Explore newer models (GPT, T5, LLaMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d1d4d7",
      "metadata": {},
      "source": [
        "## Bonus: Visualizing Embeddings\n",
        "\n",
        "Reduce 768 dimensions to 2D to see how sentences cluster by meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "449126b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sentences grouped by topic\n",
        "sentences = [\n",
        "    # Animals\n",
        "    \"The cat sleeps on the couch.\",\n",
        "    \"A dog plays in the yard.\",\n",
        "    \"The kitten chases a mouse.\",\n",
        "    # Programming\n",
        "    \"Python is great for data science.\",\n",
        "    \"JavaScript powers the web.\",\n",
        "    \"Rust is a systems language.\",\n",
        "    # Weather\n",
        "    \"It's raining outside today.\",\n",
        "    \"The sun is shining brightly.\",\n",
        "    \"Snow covers the ground.\",\n",
        "]\n",
        "\n",
        "labels = [\"animal\"] * 3 + [\"programming\"] * 3 + [\"weather\"] * 3\n",
        "colors = {\"animal\": \"red\", \"programming\": \"blue\", \"weather\": \"green\"}\n",
        "\n",
        "# Get embeddings\n",
        "embeddings = st_model.encode(sentences)\n",
        "\n",
        "# Reduce to 2D with PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(embeddings)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, (x, y) in enumerate(reduced):\n",
        "    plt.scatter(x, y, c=colors[labels[i]], s=100)\n",
        "    plt.annotate(sentences[i][:25] + \"...\", (x, y), fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.title(\"BERT Sentence Embeddings (PCA)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "\n",
        "# Legend\n",
        "for label, color in colors.items():\n",
        "    plt.scatter([], [], c=color, label=label)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
